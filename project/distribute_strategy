
tf.distribute.Strategy

딥러닝 모델에서 메모리 문제를 해결하는 데에는 두 가지 방법이 있다. 
모델을 분산시키거나 데이터셋을 분산 시키거나. ( Model Parallelism vs Data Parallelism)

여기서 Model Parallelism은
1. 분산된 모델이 참조하는 파라미터가 서로 종속적일 수 있고, (이로 인해 역전파 계산이 어려워진다)
2. 서로 다른 모델 간의 파라미터 연산 속도로 차이로 인한 병목현상 가능성이 있다

(이런 기술적인 문제로 tensorflow에서 Model Parallelism을 이용해도 만족 할만한 성능은 나오지 않는다고 한다)

출처: https://ettrends.etri.re.kr/ettrends/172/0905172001/

_____________________________________________________________________________

반면 Data Parallelism에서는 모든 워커의 전역 파라미터 갱신 및 
원격 파라미터 서버에 반영하는 식으로 파리미터 처리가 가능하다


tf.distribute.Strategy API에서 데이타셋을 자동으로 나눠준다. 그러나 이는 한쪽으로 편향된 batch를 만들 수도 있기에 
off로 돌릴 수도 있다. 대신 수동으로 각 worker에 배당되는 batch를 정할 수도 있다. 




tf.distribute.MirroredStrategy 

keras에서 쓰듯이 eager mode(그래프 없이 즉시 실행)으로도 이를 적용할 수 있지만 
tf 1.0 대에서 신나게 굴리던 그래프 형식에서 적용하는 것을 권장한다. 

Distribution Strategy에 의해 모델을 바꾸지 않고 CPU,GPU,TPU에서 실행 할 수 있게 되었다. 

분산 전략은 크게 두 가지로 나뉜다

synchronous training : 모든 worker는 각자 다른 data slice를 배당받고 각 단계마다 가중치를 서로 종합한다. 
asynchronous training : 모든 worker가 data를 가지고 각자 훈련한뒤 가중치 또한 독립적으로 파라미터 서버에 갱신한다. 



왜 쓰는가? 
 - 한번에 큰 배치를 여러 개로 나눠서 학습할 수 있음 
 - 시간 절약 



여러 개의 Worker들 간의 파라미터를 전송학 위해 Message Passing, MPI,gRPC, NCCL 등이 있다

NCCL은 Nvidia에서 나온거라 한다. 역시 Windows 하고 안친하다. (Window에서는 저 옵션이 안돌아가서 에러 발생)

_____________________________________________________________________________

tf.distribute.experimental.MultiWorkerMirroredStrategy

Mulit worker training에서는 당연하게도 여러 worker들이 일을 한다.
이 worker들 중 1명이 chef worker가 되어, 보통 0번째 worker, 추가적인 일을 한다.
체크포인트 저장, 텐서보드에 전달한 요약 파일 작성 등등 


Cluster : training set이 부분으로 나뉘어 각 worker에게 가는 정보 할당량

Task : 각 worker 들이 하게 될 나눠진 Cluster. 작업 종류, 타입 등이 나눠진다. 



ex) 앞서 말했듯이 chef worker는 좀 더 많을 일을 해야한다. 
    즉, 일반 worker들과 Job이 달라진다.  
    같은 Cluster를 안에서도 맡은 Job에 따라 다른 Task를 하게 된다.  
