# -*- coding: utf-8 -*-
"""Untitled

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12Gfkv0m3P7h_F9e1LzjjtDYwBbUKZBxj
"""

# 388p 

model.compile(loss = ['mse','mse'], loss_weights = [0.9,0.1], optimizer='sgd')
# loss_weights 를 통해 인풋의 가중치 비율을 조절할 수 있다 

# 389p

# subclassing API 

class DeepModel (keras.model) :
  def __init__(self,units = 30, activation = 'relu', **kwargs):
    super().__init__(**kwargs) # 표준 매게변수 처리 ex) name
    self.hidden1 = keras.layers.Dense(units, activation = activation)
    self.hidden2 = keras.layers.Dense(units, activation = activation) 
    self.main_output = keras.layers.Dense(1)
    self.aux_output = keras.layers.Dense(1)

  def call(self, inputs) :
    input_A = self.hidden1(input_B)
    input_B = self.hidden2(hidden1)
    concat = keras.layers.concatenate([input_A, hidden2])
    main_output = self.main_output(concat)
    aux_output = self.aux_output(hidden2)
    return main_output, aux_output 
  
model = DeepModel()

# input 클래스 객체 필요없이 call 매서드의 input 매게변수 사용 
# 유연성 -> Sequential < Funtaional < subclassing API 


# 394p tensorboard 

import os 
root_logdir = os.path.join.(os.curdir, "my logs")

def get_run_logdir() : 
  import time
  run_id = time.strftime("run_%Y_%m_%d-%H_%M_%S")
  return os.path.join(root_logdir, run_id)

run_logdir = get_run_logdir()  # ./my_logs/run_2019_06_07-15_15_22 이런 식으로 

tensorboard_cb = keras.callbacks.Tensorboard(run_logdir)
history = model.fit(X_train, y_train, epochs= 30, validation_data = (X_vaild, y_vaild), callbacks = [tensorboard_cb])



# 400p 하이퍼 파라미터 튜닝 
# 미리 만들어놓은 RandomizedSearchCV 말고도 여러 라이브러리들이 있다. 나중에 사용해보자 


# 이론적으로는 layer 하나로도 학습이 가능하다, 하지만 효율이 떨어질 것이다
# 이전 레이어가 계산한 것을 받아먹기만 하면 되니까 여러 개층으로 나뉘어진다면 훨씬 효율적으로 학습할 수 있다. 
# 그래서 다른 모델에서 기본적으로 학습한 것을 땡겨오는 transfer learning 방식이 있다. 
# 예를 들어 이미지 쪽에서 저수준 구조, 점, 방향, 선 등은 또 다시 학습하기에는 비효율적이니까 

# 스트레치 팬츠 - > 처음부터 맞는 사이즈를 찾는 것보다 일단 큰 옷을 입어보고 사이즈를 줄여나가자 

# tip. 일반적으로 뉴런 수를 늘리는 것보다 층 수를 늘리는 쪽이 이득이 많다. 

# batch_size 논쟁 - 작게 하냐 크게 하냐 , 크게 해도 결국은 결과는 비슷하다고 한다. 단지 학습 초기에 불안정하게 이리저리 튈 뿐. 


#chapter 11 

# vanishing gradient  ->  입력층의 분산보다 츌력층의 분산이 커서 마지막 층에는 거의 아무것도 도달하지 않는 현상 
# 이를 방지하기 위해 각 층의 가중치들 중에서 랜덤으로 초기화시켜버린다. 
# 이런 초기화 전략으로 글로럿, He(Relu), 르쿤(SELU) 등등이 있다
# keras default 는 글로럿 초기화 

keras.layers.Dense(10, activation = 'relu', kernel_initializer = 'he_normal') # 혹은 kernel_initializer = 'he_uniform' 으로 He 초기화를 사용할 수 있다 

# 위의 fan in 대신 fan out 기반의 균등분포 He 초기화를 사용하려면

he_avg_init = keras.initializers.VarianceScailing(scale =2., mode= 'fan_avg', distribution= 'uniform')
keras.layers.Dense(10, activation= 'sigmoid', kernel_initializer = he_avg_init)

# relu를 많이 쓰지만 모든 샘플에 대한 입력의 가중치 합이 음수가 되면 뉴런이 죽게 되는 문제가 있다. 
# 그래서 LeakyRelu 를 써서 < 0 일 상황일 때도 뉴런이 죽지 않게 한다 
# RRelu, PRelu 도 있다고 한다 
# ELU 는 다른 모든 Relu변종들의 성능을 앞질렀다고 한다 다만 지수함수를 사용하기 때문에 계산이 느리다, 훈련하는 동안에는 수렴하기 때문에 상관없지만 테스트 때에는 느릴 것이다. 

# 특정 조건 하에서는 SELU가 성능이 좋다 - 
#  1) 입력값이 표준화 되어 있어야 하고, 
#  2) hidden layer가 'lecun_normal' 로 가중치 초기화 되어야 하고,  -> z=0에서 연속적이지 않기 때문에
#  3) Sequential Dense model이여야 한다 (RNN에서는 쓰지 못한다, 그런데 CNN에서는 가능하다고 카더라)

keras.layers.Dense(10, activation= 'sigmoid', kernel_initializer = he_avg_init)
keras.layers.LeakyRelu(alpha=0.2) # 이렇게 모델에서 적용하려는 층 뒤에 추가한다 

# PRelu 같은 경우는 keras에서 직접 지원은 안하지만 셀프로 구현해서 넣을 수 있다

# Batchnormalization - 입력값의 스케일을 조정(standard scaler처럼)하고 이를 이동 시긴다 
# 적은 훈련 단계로 좋은 학습 결과를 보여준다고 논문에서는 주장한다 
# 하지만 모델의 복잡도를 크게 해 학습 시간이 늘어난다( 그런데 수렴 속도는 더 빨라지므로 최종 시간은 비슷하다고 한다) , 다만 한 번 학습이 끝나면 TFlite 변환기에서 layer를 합칠 수 있다 

keras.layers.Batchnormalization()
keras.layers.Dense(10, activation= 'sigmoid', kernel_initializer = he_avg_init)
keras.layers.Batchnormalization() # 앞이나 뒤에 배치한다, 그런데 활성화 함수를 추가한다면 이를 뒤에 둘지 앞에 둘지는 논란이 있다. 논문에서는 활정화 함수 이전에 batchnormalization을 넣는걸 추천한다

#깊은 네트워크일수록 큰 효과를 발휘한다 

# batchnormalization의 parameter 로는 momentum, axis등이 있다. 뭔지는 더 찾아봐야겠다 \
# 대부분은 당연히 이게 있다고 가정하기에 그림에서 종종 빠져있다. 알아서 넣어보자 



# 하지만 RNN에서는 batchnormalization을 사용하기 힘들기에 gradient cilpping 기법을 사용한다 

optimizer = keras.optimizers.SGD(clipvalue=1.0)
model.compile(loss = 'mse', optimizer = optimizer)

# clipvalue , clipnorm 뭔 소린지 모르겠다 






# 전이 학습 

# 그런데 왜 자꾸 딥러닝 딥러닝 하면서 정작 하위층을 입력값 받는 층에 가까운 층으로 설정하는걸까


model_A = keras.models.load_model("my_model_A.h5'")
model_B_on_A = keras.models.Sequential(model_A.layers[:-1])  # model_A의 일부 층을 공유, 그래서 B_on_A를 훈련할  때 A도 영향을 받는다. 이를 피하려면 A를 clone한다 
model_B_on_A.add(keras.layers.Dense(1, activation = 'sigmoid'))

model_A_clone = keras.models.clone_model(model_A)
model_A_clone.set_weights(model_A.get_weights())

for layer in model_B_on_A.layers[:-1] :
  layer.trainable = False

model_B_on_A.compile(loss='binary_crossentrophy', optimizer = 'sgd', metrics=['acc'])
# 처음 몇 번의 epochs 동안 재사용된 층 동결, 새로운 층만 학습, 그 다음 동결을 풀고 다시 compile 학습 진행

history  = model_B_on_A.fit(X_train_B, y_train_B, epochs = 4 , validation_data = (X_vaild_B, y_vaild_B))

for layer in model_B_on_A.layers[:-1]:
  layer.trainable = True

optimizer = keras.optimizers.SGD(lr=1e-4)
model_B_on_A.compile (loss = 'binary_crossentropy', optimizer = optimizer, mertrics=['acc'])

history = model_B_on_A.fit(X_train, y_train_B, epochs= 16, validation_data=(X_vaild_B, y_vaild_B))

# 그런데 사실 이건 잘 보이기위한 속임수였다 


# 고속 옵티마이저 
# 경사 하강법에 가속도를 붙여보자 
# 이를 제어하기 위한 parameterd는 momentum

optimizer = keras.optimizer.SGD(lr=0.001, momentum = 0.9) # 보통 0.9 두면 된다 

# NAG, AdaGrad, RMSProp, Adam, AdaMax, Nadam(NAG+Adam), 

# 텐서플로 모델 최적화 툴킷 ( TF - MOT )가 있다고 한다 

# callback 에 LearningRateScheduler를 넣어 처음에는 큰 lr로 나중에는 작은 lr로 학습을 효율적으로 진행할 수 있다. 
# 혹은 keras.optimizers.schedules 사용 


# L1, L2 규제 

# 코드를 깔끔하게 만들기 위해 python의 functools.partial 사용

from functools import partial 

RegularizedDense = partial(keras.layers.Dense, activation ='elu', 
                           kernel_initializer='he_normal', 
                           kernel_regularizer= keras.regularizers.l2(0.01))

model = keras.models.Sequential([
                                 keras.layers.Flatten(input_shape=[28,28]),
                                 RegularizedDense(300),
                                 RegularizedDense(200),
                                 RegularizedDense(10, activation = 'softmax', kernel_initializer='glotot_uniform')

])

# Dropout 

# RNN -> 20 ~ 30 %
# CNN -> 40 ~ 50 %

# 일반화 성능의 증대 
# 보통 입력층에서 세 번쨰 층까지만 Dropout 적용 
# Dropout.layer는 train 하는 동안에만 활성화 된다. 

# 그런데 dropout을 쓰면 평소보다 많은 뉴런과 연결되기 때문에 이를 보상해주기 위해  각 뉴런의 가중치에 0.n 을 곱할 필요가 있다 (보존확률(1-p))

# SELU optimizer 쓸 때에는 alpha dropout 을 써야한다 



# advanced method로는 
# 몬테 카를로 Dropout 과 맥스-노름 규제가 있다. 
# 뭔지 아직 잘 모르겠다

"""# 새 섹션"""